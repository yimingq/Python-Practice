{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Language Modelling in Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name:Yiming Qiu\n",
    "\n",
    "Student ID:806719"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>:  Friday, 17 May 2019 4pm\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 7% of mark for class (with 6% on correctness + 1% on quality and efficiency of your code)\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib and Scikit-Learn. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>. \n",
    "\n",
    "To familiarize yourself with NLTK, here is a free online book:  Steven Bird, Ewan Klein, and Edward Loper (2009). <a href=http://nltk.org/book>Natural Language Processing with Python</a>. O'Reilly Media Inc. You may also consult the <a href=https://www.nltk.org/api/nltk.html>NLTK API</a>.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll be creating an 'artificial intelligence' player for the classic Hangman word guessing game. You will need to implement several different automatic strategies based on character level language models. Your objective is to create an automatic player which makes the fewest mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hangman Game (*No implementation is needed*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a> is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then (in which case they *win*). \n",
    "\n",
    "Here's a simple version of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "        This function plays the hangman game with the provided gusser and returns the number of incorrect guesses. \n",
    "        \n",
    "        secret_word: a string of lower-case alphabetic characters, i.e., the answer to the game\n",
    "        guesser: a function which guesses the next character at each stage in the game\n",
    "            The function takes a:\n",
    "                mask: what is known of the word, as a string with _ denoting an unknown character\n",
    "                guessed: the set of characters which already been guessed in the game\n",
    "                guesser_args: additional (optional) keyword arguments, i.e., name=value\n",
    "        max_mistakes: limit on length of game, in terms of allowed mistakes\n",
    "        verbose: be chatty vs silent\n",
    "        guesser_args: keyword arguments to pass directly to the guesser function\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a human guesser allowing interactive play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    This is a simple function for manual play.\n",
    "    \"\"\"\n",
    "    print('Enter your guess:')\n",
    "    return input().lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to play hangman interactively, please set `interactive` to True. When submitting your solution, set to False so we can automatically run the whole notebook using `Run All`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play the game interactively using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    hangman('whatever', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing Test Set and Training Set (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: We will be using the words occurring in the *Brown* corpus for *training* an artificial intelligence guessing algorithm, and for *evaluating* the quality of the algorithm. Note that we are intentionally making the hangman game hard, as the AI will need to cope with test words that it has not seen before, hence it will need to learn generalisable patterns of characters to make reasonable predictions.\n",
    "\n",
    "Your first task is to compute the unique word types occurring in the *Brown* corpus, using `nltk.corpus.Brown` and the `words` method, selecting only words that are entirely comprised of alphabetic characters, and lowercasing the words. Finally, randomly shuffle (`numpy.random.shuffle`) this collection of word types, and split them into disjoint training and testing sets. The test set should contain 1000 word types, and the rest should be in the training set. Your code should print the sizes of the training and test sets.\n",
    "\n",
    "Feel free to test your own Hangman performance using `hangman(numpy.random.choice(test_set), human, 8, True)`. It is surprisingly difficult (and addictive)!\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40234\n",
      "1000\n",
      "39234\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# word_set stores all the unique word types in the Brown corpus\n",
    "word_set = []\n",
    "# test_set stores 1000 word types for testing\n",
    "test_set = []\n",
    "# training_set stores the rest word types for training\n",
    "training_set = []\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\"\"\"\n",
    "Variable:\n",
    "    word_set_real: a word set of brown corpus\n",
    "    word_set: list of word_set_real\n",
    "    \n",
    "\"\"\"\n",
    "word_set_real = set()\n",
    "for genre in brown.categories():\n",
    "    for word in brown.words(categories=genre):\n",
    "        if word.isalpha():\n",
    "            word_set_real.add(word.lower())\n",
    "word_set = list(word_set_real)\n",
    "\n",
    "test_set = word_set[:1000]\n",
    "training_set = word_set[1000:]\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(len(word_set))\n",
    "print(len(test_set))\n",
    "print(len(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(word_set) > 35000 and len(word_set) < 45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(test_set) == 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(training_set) + len(test_set) == len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    hangman(np.random.choice(test_set), human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Guesser: Random Guessing (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: To set a baseline, your first *AI* attempt will be a trivial random method. For this you should implement a guessing method, similar to the `human` method above, i.e., using the same input arguments and returning a character. Your method should randomly choose a character from the range `'a'...'z'` after excluding the characters that have already been guessed in the current game (all subsequent AI approaches should also exclude previous guesses). You might want to use `numpy.random.choice` for this purpose.\n",
    "\n",
    "To help you measure the performance of this (and later) guesser, a `test_guesser` method that takes a guesser and measures the average number of incorrect guesses made over all the words in the `test_set` is provided to you. \n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_guesser(guesser, test=test_set):\n",
    "    \"\"\"\n",
    "        This function takes a guesser and measures the average number of incorrect guesses made over all the words in the test_set. \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for word in test:\n",
    "        total += hangman(word, guesser, 26, False)\n",
    "    return total / float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of incorrect guesses:  16.688\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def random_guesser(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "        This function implements a random guesser. It returns the random guess. \n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \"\"\"\n",
    "    Variable: \n",
    "        alpha: 26 alphas\n",
    "        rest: the rest of alpha set after getting diffence with alphas in mask and guessed\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = set(list(string.ascii_lowercase))\n",
    "    mask_set = set(mask)\n",
    "    rest = alpha.difference(mask_set).difference(guessed)\n",
    "    return np.random.choice(list(rest))\n",
    "\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "# uncomment to run a single hangman game with output shown (useful for debugging)\n",
    "# hangman(np.random.choice(test_set), random_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(random_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 10 and result < 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your First AI Guesser: Unigram Guesser (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** As your first real AI, you should train a *unigram* model over the training set.  This requires you to find the frequencies of characters over all training words. Using this model, you should write a guesser that returns the character with the highest probability. Remember to exclude already guessed characters. \n",
    "\n",
    "Hint: It should be much lower than random guessing.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of incorrect guesses:  10.434\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# unigram_counts stores the frequencies of characters over all training words\n",
    "unigram_counts = Counter()\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "for word in training_set:\n",
    "    for char in word:\n",
    "        unigram_counts[char]+=1\n",
    "\n",
    "# ##\n",
    "# Your answer ENDS HERE\n",
    "# ##\n",
    "\n",
    "\n",
    "def unigram_guesser(mask, guessed, unigram_counts=unigram_counts):\n",
    "    \"\"\"\n",
    "        This function implements a unigram guesser. It returns a guess based on the unigram model. \n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \"\"\"\n",
    "    Vairable:\n",
    "        counts_list: list of alpha tuple, sorted based on unigram count\n",
    "    \"\"\"\n",
    "    alpha = set(list(string.ascii_lowercase))\n",
    "    mask_set = set(mask)\n",
    "    rest = alpha.difference(mask_set).difference(guessed)\n",
    "    counts_list = unigram_counts.most_common()\n",
    "    for char in counts_list:\n",
    "        if char[0] in rest:\n",
    "            return char[0]\n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "# hangman(np.random.choice(test_set), unigram_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(unigram_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 5 and result < 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your Second AI Guesser: Length-based Unigram Guesser (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** The length of the secret word is an important clue that we might exploit. Different length words tend to have different distributions over characters, e.g., short words are less likely to have suffixes or prefixes. You should incorporate this idea by conditioning the unigram model on the length of the secret word, i.e., having a *different* unigram model for each length of the words. You will need to be a little careful at test time, to be robust to the (unlikely) situation that you encounter a word length that you didn't see in training. You need to decide on how to handle this situation.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "\n",
      "defaultdict(<class 'collections.Counter'>, {7: Counter({'e': 5323, 'a': 3550, 's': 3452, 'r': 3398, 'i': 3337, 'n': 3045, 't': 2660, 'o': 2508, 'l': 2491, 'd': 2064, 'c': 1675, 'u': 1498, 'g': 1488, 'm': 1235, 'p': 1228, 'h': 1104, 'b': 940, 'y': 745, 'f': 676, 'k': 584, 'w': 521, 'v': 459, 'z': 154, 'x': 111, 'j': 95, 'q': 81}), 11: Counter({'e': 2976, 'i': 2772, 'n': 2282, 't': 2167, 'a': 2038, 's': 2020, 'r': 1941, 'o': 1726, 'l': 1338, 'c': 1239, 'u': 864, 'd': 809, 'p': 783, 'm': 743, 'g': 641, 'h': 524, 'b': 425, 'y': 418, 'f': 294, 'v': 293, 'k': 136, 'w': 118, 'x': 74, 'z': 70, 'q': 45, 'j': 27}), 8: Counter({'e': 5587, 'i': 3889, 's': 3691, 'a': 3659, 'r': 3485, 'n': 3485, 't': 2913, 'o': 2768, 'l': 2617, 'd': 2211, 'c': 1938, 'u': 1568, 'g': 1524, 'm': 1298, 'p': 1236, 'h': 1156, 'b': 850, 'f': 714, 'y': 712, 'v': 537, 'k': 512, 'w': 468, 'z': 108, 'x': 107, 'j': 85, 'q': 82}), 5: Counter({'e': 2196, 's': 1824, 'a': 1811, 'r': 1368, 'o': 1319, 'l': 1196, 'i': 1145, 't': 1075, 'n': 1021, 'd': 773, 'c': 700, 'u': 673, 'h': 581, 'm': 570, 'y': 547, 'p': 524, 'b': 494, 'g': 459, 'k': 378, 'f': 344, 'w': 303, 'v': 235, 'z': 87, 'j': 77, 'x': 61, 'q': 29}), 10: Counter({'e': 4338, 'i': 3629, 'n': 3082, 's': 2908, 'a': 2792, 't': 2789, 'r': 2665, 'o': 2389, 'l': 1953, 'c': 1679, 'd': 1414, 'u': 1295, 'g': 1094, 'p': 1071, 'm': 1052, 'h': 833, 'b': 619, 'y': 552, 'f': 466, 'v': 444, 'w': 212, 'k': 178, 'z': 111, 'x': 95, 'q': 65, 'j': 55}), 6: Counter({'e': 4259, 'a': 2704, 's': 2610, 'r': 2534, 'i': 2039, 'n': 2002, 'o': 1981, 'l': 1960, 't': 1854, 'd': 1630, 'u': 1148, 'c': 1074, 'm': 943, 'g': 894, 'h': 876, 'p': 855, 'b': 777, 'y': 715, 'f': 498, 'k': 451, 'w': 433, 'v': 395, 'z': 124, 'j': 106, 'x': 93, 'q': 51}), 13: Counter({'i': 1290, 'e': 1043, 'n': 1024, 't': 969, 'a': 912, 'o': 838, 's': 807, 'r': 749, 'c': 569, 'l': 541, 'p': 362, 'm': 346, 'u': 335, 'd': 260, 'g': 245, 'y': 205, 'h': 202, 'b': 142, 'v': 116, 'f': 114, 'x': 35, 'z': 35, 'k': 28, 'w': 23, 'q': 20, 'j': 9}), 12: Counter({'i': 1899, 'e': 1896, 'n': 1558, 't': 1464, 'a': 1429, 's': 1320, 'r': 1263, 'o': 1233, 'l': 965, 'c': 905, 'u': 593, 'p': 547, 'm': 525, 'd': 506, 'g': 389, 'h': 384, 'y': 300, 'b': 258, 'f': 197, 'v': 196, 'z': 70, 'k': 55, 'w': 52, 'x': 39, 'q': 35, 'j': 18}), 9: Counter({'e': 5392, 'i': 3935, 's': 3458, 'n': 3457, 'a': 3438, 'r': 3242, 't': 3071, 'o': 2621, 'l': 2312, 'c': 1911, 'd': 1855, 'u': 1445, 'g': 1368, 'm': 1241, 'p': 1159, 'h': 1010, 'b': 752, 'y': 652, 'f': 588, 'v': 528, 'k': 375, 'w': 363, 'z': 134, 'x': 123, 'q': 89, 'j': 67}), 4: Counter({'e': 900, 'a': 866, 's': 693, 'o': 680, 'l': 548, 'i': 541, 'r': 520, 't': 484, 'n': 474, 'd': 360, 'u': 346, 'm': 298, 'h': 289, 'p': 284, 'b': 263, 'c': 260, 'k': 230, 'g': 221, 'w': 190, 'y': 181, 'f': 172, 'v': 97, 'j': 62, 'z': 46, 'x': 35, 'q': 8}), 3: Counter({'a': 238, 'e': 200, 'o': 181, 'i': 124, 'n': 115, 't': 115, 's': 111, 'p': 104, 'm': 102, 'u': 99, 'r': 98, 'd': 97, 'b': 91, 'l': 77, 'y': 77, 'g': 76, 'h': 76, 'w': 74, 'c': 67, 'f': 50, 'k': 31, 'j': 29, 'v': 24, 'x': 23, 'z': 10, 'q': 6}), 16: Counter({'i': 164, 't': 127, 'a': 108, 'n': 106, 'o': 103, 'e': 102, 'r': 93, 's': 87, 'l': 87, 'c': 60, 'p': 45, 'h': 35, 'm': 35, 'u': 31, 'y': 31, 'd': 27, 'g': 22, 'b': 11, 'f': 10, 'z': 9, 'v': 6, 'x': 6, 'k': 4, 'q': 2, 'w': 1}), 14: Counter({'i': 715, 'e': 573, 'n': 549, 't': 527, 'a': 463, 's': 461, 'o': 454, 'r': 398, 'l': 335, 'c': 305, 'p': 178, 'u': 172, 'm': 160, 'd': 148, 'h': 131, 'y': 119, 'g': 116, 'b': 68, 'v': 63, 'f': 62, 'z': 35, 'x': 12, 'k': 11, 'w': 10, 'q': 8, 'j': 3}), 2: Counter({'a': 26, 'e': 20, 'm': 19, 's': 19, 'o': 19, 'i': 16, 'l': 13, 'h': 12, 'p': 12, 'n': 11, 'd': 11, 'c': 11, 'u': 11, 'f': 10, 'b': 9, 'r': 9, 't': 9, 'v': 7, 'w': 6, 'g': 6, 'y': 6, 'j': 5, 'k': 5, 'x': 3, 'q': 2, 'z': 1}), 15: Counter({'i': 404, 't': 289, 'n': 279, 'o': 255, 'e': 251, 'a': 238, 's': 214, 'r': 204, 'l': 170, 'c': 168, 'p': 111, 'y': 80, 'u': 75, 'm': 74, 'd': 72, 'h': 70, 'g': 52, 'f': 26, 'b': 24, 'v': 20, 'z': 18, 'k': 8, 'x': 8, 'w': 5, 'q': 4, 'j': 1}), 19: Counter({'o': 16, 'i': 14, 't': 13, 'a': 11, 'n': 11, 'r': 9, 'e': 6, 'l': 6, 'c': 5, 'm': 3, 'p': 3, 'd': 3, 'g': 3, 'u': 3, 'z': 2, 'h': 2, 's': 2, 'y': 1, 'f': 1}), 1: Counter({'d': 1, 'j': 1, 'g': 1, 'k': 1, 'r': 1, 'c': 1, 'h': 1, 'o': 1, 'v': 1, 'b': 1, 'p': 1, 'w': 1, 'i': 1, 'a': 1, 'e': 1, 's': 1, 'n': 1, 'y': 1, 'x': 1, 'm': 1, 'l': 1, 'u': 1, 'f': 1, 't': 1}), 17: Counter({'i': 90, 't': 77, 'e': 69, 'a': 69, 'n': 64, 'r': 61, 'o': 56, 'l': 41, 's': 40, 'c': 38, 'p': 27, 'h': 26, 'm': 26, 'd': 24, 'y': 20, 'u': 18, 'g': 12, 'z': 9, 'b': 5, 'v': 3, 'f': 3, 'w': 1, 'j': 1, 'k': 1, 'x': 1}), 18: Counter({'i': 48, 't': 44, 'e': 37, 's': 35, 'r': 34, 'o': 33, 'n': 29, 'a': 25, 'c': 24, 'l': 23, 'p': 14, 'h': 13, 'd': 12, 'y': 10, 'm': 9, 'u': 8, 'g': 6, 'f': 4, 'b': 2, 'k': 2, 'v': 1, 'z': 1}), 21: Counter({'o': 6, 'c': 4, 'i': 3, 'm': 3, 'e': 3, 'l': 3, 'r': 3, 'p': 3, 'h': 3, 's': 3, 'a': 3, 'u': 1, 'n': 1, 't': 1, 'y': 1, 'g': 1}), 22: Counter({'e': 4, 'l': 3, 'n': 3, 'a': 2, 's': 2, 'k': 1, 'y': 1, 'b': 1, 'z': 1, 'u': 1, 'f': 1, 'o': 1, 't': 1}), 20: Counter({'i': 5, 't': 4, 'n': 3, 'o': 2, 'a': 2, 's': 1, 'u': 1, 'l': 1, 'z': 1})})\n",
      "\n",
      "Average number of incorrect guesses:  10.423\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# unigram_counts_by_length stores a dictionary, mapping word length to the frequencies of characters of words with that word length\n",
    "unigram_counts_by_length = defaultdict(Counter)\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "for word in training_set:\n",
    "    for char in word:\n",
    "        unigram_counts_by_length[len(word)][char]+=1\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "\n",
    "lengths = sorted(unigram_counts_by_length.keys())\n",
    "max_length = lengths[-1]\n",
    "print(lengths)\n",
    "print()\n",
    "print(unigram_counts_by_length)\n",
    "\n",
    "def unigram_length_guesser(mask, guessed, counts=unigram_counts_by_length):\n",
    "    \"\"\"\n",
    "        This function implements a length-based unigram guesser. It returns a guess based on the length-based unigram model. \n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    mask_len = len(mask)\n",
    "    \n",
    "    alpha = set(list(string.ascii_lowercase))\n",
    "    mask_set = set(mask)\n",
    "    rest = alpha.difference(mask_set).difference(guessed)\n",
    "    \n",
    "\n",
    "    counts_list = unigram_counts_by_length[mask_len].most_common()\n",
    "    for char in counts_list:\n",
    "        if char[0] in rest:\n",
    "            return char[0]\n",
    "        \n",
    "    \"\"\"in case of has the charactor that not in counter, use general unigram method to return a alpha\"\"\"\n",
    "\n",
    "    unigram_counts_list = unigram_counts.most_common()\n",
    "    for char in unigram_counts_list:\n",
    "        if char[0] in rest:\n",
    "            return char[0]\n",
    "       \n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "# hangman(np.random.choice(test_set), unigram_length_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(unigram_length_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 5 and result < 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Third AI Guesser: Bigram Guesser (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Now for the next challenge, using a *bigram* language model over characters. The order of characters is obviously important, yet this wasn't incorporated in any of the above models. Knowing that the word has the sequence `n _ s s` is a pretty strong clue that the missing character might be `e`. Similarly the distribution over characters that start or end a word are highly biased (e.g., toward common prefixes and suffixes, like *un-*, *-ed* and *-ly*).\n",
    "\n",
    "You should develop a *bigram* language model over characters, train this over the training words (being careful to handle the start of each word properly, e.g., by padding with a sentinel symbol `$`.) You should use *linear interpolation* to smooth between the higher order and lower order models, and you will have to decide how to weight each component (be reminded that all probabilities should sum to 1).\n",
    "\n",
    "Your bigram guesser should apply your language model to each blank position in the secret word by using the left context as is known. E.g., in the partial word `$ _ e c _ e _ _` we know the left context for the first three blanks, but have no known left context for the last blank. Using a bigram language model, we are able to apply it to the first three blanks only. You should then select the character with the highest probability of predicting the most number of correct entries over the blanks. \n",
    "\n",
    "Do you see any improvement over the unigram guessers above?\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of incorrect guesses:  8.86\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "uni_prob = {}\n",
    "\n",
    "uni_sum = sum(unigram_counts.values())+len(training_set)\n",
    "\n",
    "for char in unigram_counts:\n",
    "    uni_prob[char]= unigram_counts[char]\n",
    "uni_prob['$']=len(training_set)\n",
    "\n",
    "bigram_counts = defaultdict(Counter)\n",
    "\n",
    "for word in training_set:\n",
    "    word = \"$\"+word\n",
    "    for i in range(len(word)-1):\n",
    "        bigram_counts[word[i]][word[i+1]]+=1\n",
    "\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "def bigram_guesser(mask, guessed, counts=bigram_counts): # add extra default arguments if needed\n",
    "    \"\"\"\n",
    "        This function implements a bigram guesser. It returns a guess based on the bigram model using linear interpolation.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    \"\"\"\n",
    "    Variables:\n",
    "        alpha: 26 alphas\n",
    "        rest: rest of alphas after filtering the mask and guessed alphas\n",
    "        search_list: list of first charactor of bigram elements\n",
    "        prob_all: dictionary of each 26 alphas and their probabilities\n",
    "        para: bigram lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = set(list(string.ascii_lowercase))\n",
    "    mask_set = set(mask)\n",
    "    rest = alpha.difference(mask_set).difference(guessed)    \n",
    "    \n",
    "    mask_str = ''.join(mask)\n",
    "    mask_str = \"$\"+mask_str\n",
    "    search_list = []\n",
    "    \n",
    "    for i in range(len(mask_str)):\n",
    "        if mask_str[i]=='_':\n",
    "            if mask_str[i-1]!= '_':\n",
    "                search_list.append(mask_str[i-1]) \n",
    "\n",
    "    prob_all = {}\n",
    "    \n",
    "    para = 0.9\n",
    "    \n",
    "    \"\"\"\n",
    "    local variables: \n",
    "        p_temp: sum of probabilities for a alpha in all prediction positions in mask string\n",
    "    \"\"\"\n",
    "    for second_char in alpha:\n",
    "        p_temp = 0\n",
    "        \n",
    "        for first_char in search_list:\n",
    "            if second_char in bigram_counts[first_char]:\n",
    "                \n",
    "                # based on the method of linear interpolation\n",
    "                p_temp += para*bigram_counts[first_char][second_char]/uni_prob[first_char]+(1-para)*(uni_prob[second_char]/uni_sum)\n",
    "            else:\n",
    "                p_temp += (1-para)*(uni_prob[second_char]/uni_sum)\n",
    "\n",
    "        # store the average probabilites into prob_all\n",
    "        prob_all[second_char] = p_temp/len(search_list)\n",
    "        \n",
    "    #now reverse sort the prob_all based on the probabilities\n",
    "    sorted_prob = sorted(prob_all.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    \n",
    "    for sorted_tuple in sorted_prob:\n",
    "        if sorted_tuple[0] in rest:\n",
    "            return sorted_tuple[0]\n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "# hangman(np.random.choice(test_set), bigram_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(bigram_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result < 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Your Own AI Guesser (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** You should try to develop a more effective AI, `my_ai_guesser`, for hangman. Feel free to engage your creativity here! Possibilities include better conditioning on the length of the word, fancier smoothing methods, and using ngram models. Ensure you report the test performance of your guesser. Have fun!\n",
    "\n",
    "You will be marked based on the explanation of your approach and its accuracy. \n",
    "\n",
    "(1 mark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of incorrect guesses:  8.827\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "use partal Length-based Bigram Guesser.\n",
    "For the mask with has a common length, here will use Length-based Bigram Guesser since we already train \n",
    "a lot 'common length'word. \n",
    "For the word has uncommon length (too long or too short), will use bigram guesser we implemented before\n",
    "\n",
    "To get the common length, count the number for different length bigram models. \n",
    "\n",
    "As to smoothing for Length-based Bigram Guesser, here uses Laplace smoothing since most of time it could\n",
    "value from common word bigram model\n",
    "\n",
    "This method could get better result than normal bigram guessor\n",
    "\n",
    "Variables:\n",
    "    new_bi_all: a dictionary, key: length, value: defaultdict(Counter). For value, stores two charactors and counts\n",
    "    prob_all: 26 alphas and their probabilities\n",
    "    search_list: a list of the first charctors before the charctor we aim to predict\n",
    "    bigram_list: a list of uncommon word length, used to filter the case of using normal bigram guessor\n",
    "    sorted_prob: a sorted tuple, get from reversed sorting 'prob_all'\n",
    "    \n",
    "\"\"\"\n",
    "import string\n",
    "\n",
    "new_bi_all = {}\n",
    "for i in lengths:\n",
    "    new_bi_all[i]= defaultdict(Counter)\n",
    "    \n",
    "bi_sum = 0\n",
    "for word in training_set:\n",
    "    word = \"$\"+word\n",
    "    for i in range(len(word)-1):\n",
    "        new_bi_all[len(word)-1][word[i]][word[i+1]]+=1\n",
    "\n",
    "\n",
    "def my_ai_guesser(mask, guessed):\n",
    "    \n",
    "    \n",
    "    alpha = set(list(string.ascii_lowercase))\n",
    "    mask_set = set(mask)\n",
    "    rest = alpha.difference(mask_set).difference(guessed)    \n",
    "    \n",
    "    mask_str = ''.join(mask)\n",
    "    mask_str = \"$\"+mask_str\n",
    "    mask_len = len(mask_str)\n",
    "    \n",
    "    search_list=[]\n",
    "    for i in range(len(mask_str)):\n",
    "        if mask_str[i]=='_':\n",
    "            if mask_str[i-1]!= '_':\n",
    "                search_list.append(mask_str[i-1]) \n",
    "                \n",
    "    prob_all = {}\n",
    "    \n",
    "    bigram_list = []\n",
    "    \n",
    "    # get uncommon length word\n",
    "    \n",
    "    for length in new_bi_all:\n",
    "        if len(new_bi_all[length]) - len(alpha)<0:\n",
    "            bigram_list.append(length)\n",
    "        \n",
    "    # for uncommon lenght words, use normal bigram guesser\n",
    "    if mask_len in bigram_list:\n",
    "        return bigram_guesser(mask, guessed, counts=bigram_counts)\n",
    "    \n",
    "    for second_char in alpha:\n",
    "        \n",
    "        p_temp = 0\n",
    "        for first_char in search_list:\n",
    "            if first_char in new_bi_all[mask_len]:\n",
    "                if second_char in new_bi_all[mask_len][first_char]:\n",
    "                    p_temp += new_bi_all[mask_len][first_char][second_char]+1\n",
    "                else:\n",
    "                    p_temp += 1\n",
    "            else:\n",
    "                p_temp+=1\n",
    "                \n",
    "        prob_all[second_char] = p_temp/len(search_list)\n",
    "    \n",
    "    \n",
    "    sorted_prob = sorted(prob_all.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    \n",
    "    for sorted_tuple in sorted_prob:\n",
    "        if sorted_tuple[0] in rest:\n",
    "            return sorted_tuple[0]\n",
    "\n",
    "    \n",
    "#      return random_guesser(mask, guessed)\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "# hangman(np.random.choice(test_set), my_ai_guesser, 1, True)\n",
    "result = test_guesser(my_ai_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Explain your approach and discuss your result below. Please keep it brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
