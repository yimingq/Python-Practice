{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Preprocessing and Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Yiming Qiu\n",
    "\n",
    "Student ID: 806719"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: Friday, 29 Mar 2019 4pm\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day (both week and weekend days counted)\n",
    "\n",
    "<b>Marks</b>: 6% of mark for class (with 5% on correctness + 1% on quality and efficiency of your code)\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages (the packages listed above are all fine to use); if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "To familiarize yourself with NLTK, here is a free online book:  Steven Bird, Ewan Klein, and Edward Loper (2009). <a href=http://nltk.org/book>Natural Language Processing with Python</a>. O'Reilly Media Inc. You may also consult the <a href=https://www.nltk.org/api/nltk.html>NLTK API</a>.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll be using documents from a Wall Street Journal text corpus to create a space efficient inverted index capable of fast TF-IDF query processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we will be using documents from a Wall Street Journal text corpus. The corpus can be downloaded with the commands below. Each line contains <i>one</i> document which you should tokenize and stem using tools provided by NLTK. Some of the steps below are already provided whereas others have to be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Run the code below to download documents from a Wall Street Journal text corpus. <b><i>No implementation is needed.</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "fname = 'wsta_col_20k.gz'\n",
    "my_file = Path(fname)\n",
    "if not my_file.is_file():\n",
    "    url = 'https://trevorcohn.github.io/comp90042/resources/' + fname\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Save to the current directory\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Run the code below to read raw documents, one document per line. <b><i>No implementation is needed.</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "John Blair & Co. is close to an agreement to sell its TV station advertising representation operation and program production unit to an investor group led by James H. Rosenfield, a former CBS Inc. executive, industry sources said. Industry sources put the value of the proposed acquisition at more than $100 million. John Blair was acquired last year by Reliance Capital Group Inc., which has been divesting itself of John Blair's major assets. John Blair represents about 130 local television stations in the placement of national and other advertising. Mr. Rosenfield stepped down as a senior executive vice president of CBS Broadcasting in December 1985 under a CBS early retirement program. Neither Mr. Rosenfield nor officials of John Blair could be reached for comment. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "raw_docs = []\n",
    "with gzip.open(fname, 'rt') as f:\n",
    "    for raw_doc in f:\n",
    "        raw_docs.append(raw_doc)\n",
    "\n",
    "print(len(raw_docs))\n",
    "print(raw_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Here, you will do preprocessing. You should *tokenize* each document, *stem* and *lowercase* each token using NLTK `word_tokenize` and `PorterStemmer`, and create a *vocabulary* for all the terms (normalized types). Each term should be assigned a unique ID. Note that we are not doing any stop word removal. The vocabulary should be built as a Python *map*, mapping from all the terms $M$ to their term IDs (integers of $[0..M-1]$). The processing may take a few minutes. \n",
    "\n",
    "You may check the section, <i>\"For your testing\"</i>, below for the expected output.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 20000\n",
      "Number of unique terms = 103193\n",
      "Number of tokens = 9140697\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# processed_docs stores the list of processed docs\n",
    "processed_docs = []\n",
    "# vocab contains (term, term id) pairs\n",
    "vocab = {}\n",
    "# total_tokens stores the total number of tokens\n",
    "total_tokens = 0\n",
    "\n",
    "# TODO: iterate over docs, tokenize, stem and add to vocab and assign ID if new token\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "for raw_doc in raw_docs:\n",
    "    \n",
    "    # norm_doc stores the normalized tokens of a doc\n",
    "    norm_doc = []\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    # NLTK version: 3.4\n",
    "    word_token=nltk.word_tokenize(raw_doc)\n",
    "    for word in word_token:\n",
    "        stem_word= stemmer.stem(word)\n",
    "        lower_stem_words=stem_word.lower()\n",
    "        norm_doc.append(lower_stem_words)\n",
    "        total_tokens+=1\n",
    "\n",
    "    normed_word_set = set(norm_doc)\n",
    "    for word in normed_word_set:\n",
    "        if word not in vocab:\n",
    "            vocab[word]=len(vocab)\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    processed_docs.append(norm_doc)\n",
    "\n",
    "    \n",
    "print(\"Number of documents = {}\".format(len(processed_docs)))\n",
    "print(\"Number of unique terms = {}\".format(len(vocab)))\n",
    "print(\"Number of tokens = {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(processed_docs) == 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(vocab) > 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, you should build a Python `Counter` to count the term frequencies of each document. For each document, the counter should map the terms to term frequencies. All the counters (for all documents) should be stored in a list called *doc_term_freqs*.\n",
    "\n",
    "For example, here is a document:\n",
    "\n",
    ">the old night keeper keeps the keep in the town. in the big old house in the big old gown. The house in the town had the big old keep where the old night keeper never did sleep. The keeper keeps the keep in the night and keeps in the dark and sleeps in the light.\n",
    "\n",
    "After the tokenisation and stemming, a counter as below should be built for the document:\n",
    "\n",
    "`\n",
    "Counter({'the': 14, 'in': 7, 'keep': 6, 'old': 5, '.': 4, 'night': 3, 'keeper': 3, 'big': 3, 'town': 2, 'hous': 2, 'sleep': 2, 'and': 2, 'gown': 1, 'had': 1, 'where': 1, 'never': 1, 'did': 1, 'dark': 1, 'light': 1})\n",
    "`\n",
    "\n",
    "You may check the section, <i>\"For your testing\"</i>, below for the expected *doc_term_freqs*.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "Counter({'.': 6, 'john': 5, 'blair': 5, 'of': 5, 'to': 3, 'rosenfield': 3, ',': 3, 'a': 3, 'cb': 3, 'the': 3, 'an': 2, 'station': 2, 'advertis': 2, 'and': 2, 'program': 2, 'group': 2, 'by': 2, 'inc.': 2, 'execut': 2, 'industri': 2, 'sourc': 2, 'in': 2, 'mr.': 2, '&': 1, 'co.': 1, 'is': 1, 'close': 1, 'agreement': 1, 'sell': 1, 'it': 1, 'tv': 1, 'represent': 1, 'oper': 1, 'product': 1, 'unit': 1, 'investor': 1, 'led': 1, 'jame': 1, 'h.': 1, 'former': 1, 'said': 1, 'put': 1, 'valu': 1, 'propos': 1, 'acquisit': 1, 'at': 1, 'more': 1, 'than': 1, '$': 1, '100': 1, 'million': 1, 'wa': 1, 'acquir': 1, 'last': 1, 'year': 1, 'relianc': 1, 'capit': 1, 'which': 1, 'ha': 1, 'been': 1, 'divest': 1, 'itself': 1, \"'s\": 1, 'major': 1, 'asset': 1, 'repres': 1, 'about': 1, '130': 1, 'local': 1, 'televis': 1, 'placement': 1, 'nation': 1, 'other': 1, 'step': 1, 'down': 1, 'as': 1, 'senior': 1, 'vice': 1, 'presid': 1, 'broadcast': 1, 'decemb': 1, '1985': 1, 'under': 1, 'earli': 1, 'retir': 1, 'neither': 1, 'nor': 1, 'offici': 1, 'could': 1, 'be': 1, 'reach': 1, 'for': 1, 'comment': 1})\n",
      "Counter({'the': 9, 'of': 4, ',': 4, 'soviet': 3, 'to': 3, 'bank': 3, 'and': 3, 'said': 2, 'it': 2, 'an': 2, 'with': 2, 'institut': 2, 'in': 2, 'is': 2, 'aid': 2, '.': 2, 'banqu': 1, 'de': 1, \"l'union\": 1, 'europeen': 1, 'sign': 1, 'agreement': 1, 'two': 1, 'union': 1, 'that': 1, 'design': 1, 'format': 1, 'joint': 1, 'ventur': 1, 'under': 1, 'new': 1, 'rule': 1, 'french': 1, 'arm': 1, 'state-own': 1, 'credit': 1, 'industri': 1, '&': 1, 'commerci': 1, 'financi': 1, 'group': 1, 'will': 1, 'work': 1, 'state': 1, 'for': 1, 'foreign': 1, 'trade': 1, '``': 1, 'creat': 1, 'a': 1, 'bilater': 1, 'whose': 1, 'aim': 1, 'promot': 1, 'creation': 1, 'function': 1, 'financ': 1, 'mixed-capit': 1, 'compani': 1, 'u.s.s.r': 1, \"''\": 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# doc_term_freqs stores the counters (mapping terms to term frequencies) of all documents\n",
    "doc_term_freqs = []\n",
    "\n",
    "# TODO iterate over document and for each document produce the term frequency map and store in the list\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "for doc in processed_docs:\n",
    "    cnt = Counter()\n",
    "    for word in doc:\n",
    "        cnt[word]+=1\n",
    "    doc_term_freqs.append(cnt)\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(len(doc_term_freqs))\n",
    "print(doc_term_freqs[0])\n",
    "print(doc_term_freqs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(doc_term_freqs) == 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(doc_term_freqs[0][\"blair\"] == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(doc_term_freqs[100][\"bank\"] == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inverted Index (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Run the code below to create an `InvertedIndex` class using `vocab` and `doc_term_freqs` that you built earlier. <b><i>No implementation is needed.</i></b>\n",
    "\n",
    "Our `InvertedIndex` class contains <b>six</b> components:\n",
    "\n",
    "1. The vocabulary `vocab`, which will be used to map query terms to term ids\n",
    "2. The length of each document,  `doc_len`\n",
    "3. `doc_ids` is a list indexed by term IDs. For each term ID, it stores a list of document ids of all documents containing that term\n",
    "4. `doc_term_freqs` is a list indexed by term IDs. For each term ID, it stores a list of document term frequencies $f_{d,t}$ (how often a document $d$ contains the term $t$) of the corresponding documents stored in `doc_ids`\n",
    "5. `doc_freqs` is a list indexed by term IDs. For each term ID, it stores the document frequency $f_t$ indicating the number of documents containing one or more occurrences of term $t$;\n",
    "6. Two integers `total_num_docs` and `max_doc_len` store the total number of documents and the maximum document length\n",
    "\n",
    "These values will be used in the code below. Note that some of these components are for display purposes to verify that your implementation was correctly processing the text collection, but won't be used in TF-IDF scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents = 20000\n",
      "number of terms = 103193\n",
      "longest document length = 10576\n",
      "uncompressed space usage MiB = 58.187\n"
     ]
    }
   ],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # this function assumes each integer is stored using 8 bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list) * 8\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list) * 8\n",
    "        return space_usage\n",
    "    \n",
    "\n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "# print inverted index stats\n",
    "print(\"documents = {}\".format(invindex.num_docs()))\n",
    "print(\"number of terms = {}\".format(invindex.num_terms()))\n",
    "print(\"longest document length = {}\".format(invindex.max_doc_len))\n",
    "print(\"uncompressed space usage MiB = {:.3f}\".format(invindex.space_in_bytes() / (1024.0 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, you will use the `InvertedIndex` class to compute the TF-IDF similarity scores for the documents given a simple query $Q$.\n",
    "\n",
    "Here is a simplified formula for computing TF-IDF similarity scores:\n",
    "\n",
    "\\begin{equation*}\n",
    "Score(Q,d) = \\frac{1}{\\sqrt{|d|}} \\times \\sum_{i=1}^q \\log(1 + f_{d,t}) * \\log( \\frac{N}{f_t} ) \n",
    "\\end{equation*}\n",
    "\n",
    "where $Q$ corresponds to a query containing $q$ query terms, $|d|$ corresponds to the length of the document (in words), $f_{d,t}$ corresponds to the frequency of term $t$ in document $d$, $N$ corresponds to the number of documents in the collection, and $f_t$ corresponds to the document frequency of term $t$. All these information are available in the `InvertedIndex` class. Note that the formulation of TF-IDF is a little different to the formula for TF-IDF shown in the lectures. We have adapted the formulation here to allow for a simpler implementation, e.g., avoiding the need for repeated passes over the dataset. (All manner of variants of TF-IDF exist in practise.)\n",
    "\n",
    "You should implement the `query_tfidf` function. The `query_tfidf` function should take a query and an inverted index and output the top $k$ highest scoring documents. \n",
    "\n",
    "For example, here is a query.\n",
    "\n",
    "> south korea production\n",
    "\n",
    "Here is a sample result.\n",
    "\n",
    "> RANK  1  DOCID  176  SCORE  0.426  CONTENT  South Korea rose 1% in February from a year earlier, the\n",
    "> \n",
    "> ...\n",
    "\n",
    "You may check the section, <i>\"For your testing\"</i>, below for the expected output.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK  1 DOCID     1084 SCORE 1.210 CONTENT Unemployment in South Korea fell to 3.8% of the labor force last year from \n",
      "RANK  2 DOCID      905 SCORE 1.127 CONTENT Seasonally adjusted industrial production in South Korea increased nearly 2\n",
      "RANK  3 DOCID     5612 SCORE 1.056 CONTENT Consumer prices in South Korea rose 2.4% in April from a year-earlier and 0\n",
      "RANK  4 DOCID     9126 SCORE 0.998 CONTENT Foreign investment in South Korea totaled $278 million in 1987's first quar\n",
      "RANK  5 DOCID    17960 SCORE 0.936 CONTENT Henley Group Inc. said its M.W. Kellogg Co. unit received a contract to des\n",
      "RANK  6 DOCID     1760 SCORE 0.926 CONTENT Consumer prices in South Korea rose 1% in February from a year earlier, the\n",
      "RANK  7 DOCID     4132 SCORE 0.926 CONTENT South Korea's trade deficit with Japan grew to a record $629 million in Apr\n",
      "RANK  8 DOCID    17826 SCORE 0.923 CONTENT South Korea revised its 1986 current-account surplus to $5 billion from the\n",
      "RANK  9 DOCID    15803 SCORE 0.911 CONTENT South Korea's 1986 trade surplus with the U.S. was revised upward to $7.41 \n",
      "RANK 10 DOCID    10664 SCORE 0.889 CONTENT South Korea's economy, aided by brisk exports, grew an inflation-adjusted 1\n"
     ]
    }
   ],
   "source": [
    "from math import log, sqrt\n",
    "\n",
    "# given a query and an index returns a list of the k highest scoring documents as tuples containing <docid,score>\n",
    "def query_tfidf(query, index, k=10):\n",
    "    \n",
    "    # scores stores doc ids and their scores\n",
    "    scores = Counter()\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    # remove the words which not exist in documents\n",
    "    cleaned_query=[]\n",
    "    for word in query:\n",
    "        try:\n",
    "            test = invindex.docids(word)\n",
    "            cleaned_query.append(word)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    \"\"\"\n",
    "    tf-idf algorithm implementation\n",
    "    variable:\n",
    "        d:the length of the document\n",
    "        index_doc: index of list of invert index of documents\n",
    "        N: number of docs\n",
    "        fdt: word frequency in a document\n",
    "    \"\"\"    \n",
    "    N=index.num_docs()\n",
    "    for word in cleaned_query:\n",
    "        for doc in invindex.docids(word):\n",
    "            add=0\n",
    "            d=invindex.doc_len[doc]\n",
    "\n",
    "            index_doc= invindex.docids(word).index(doc)\n",
    "            fdt = invindex.freqs(word)[index_doc]\n",
    "\n",
    "#           formula implementation\n",
    "            add+=log(1+fdt)*log(N/invindex.f_t(word))\n",
    "            score=(1/(sqrt(d)))*add\n",
    "            scores[doc]+=score\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    \n",
    "    return scores.most_common(k)\n",
    "\n",
    "\n",
    "# We output some statistics from our index\n",
    "query = \"south korea production\"\n",
    "stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "results = query_tfidf(stemmed_query, invindex)\n",
    "for rank, res in enumerate(results):\n",
    "    # e.g RANK 1 DOCID 176 SCORE 0.426 CONTENT South Korea rose 1% in February from a year earlier, the\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],raw_docs[res[0]][:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 1: DOCID\n",
    "assert(results[0][0] > 500 and results[0][0] < 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 1: SCORE\n",
    "assert(results[0][1] > 0.5 and results[0][1] < 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vbyte compression and decompression (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will reduce the space usage of the inverted index by compression. We will <i>compress</i> the `doc_ids` and `doc_term_freqs` lists in the inverted index using <b>vbyte</b> compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: You should implement two methods to perform vbyte compression and decompression as described in the lecture slides. The method signatures are provided below. \n",
    "\n",
    "- The first method `vbyte_encode(num)` should receive a number as an integer and produces a list of output bytes encoding the number. \n",
    "- The second method `vbyte_decode(input_bytes, idx)` should receive a list of input bytes and an offset into that list where the decompression should start. It returns the decoded number and the number of bytes consumed to decode the number.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vbyte_encode(num):\n",
    "\n",
    "    # out_bytes stores a list of output bytes encoding the number\n",
    "    out_bytes = []\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \"\"\"\n",
    "    Encode the number into bytes(represented by integer smaller than 256 in this case)\n",
    "    Variable: \n",
    "        single_byte: 1 byte(integer smaller than 128)\n",
    "    \"\"\"\n",
    "    while num>=128:\n",
    "        single_byte= num%128\n",
    "        num = num//128\n",
    "        out_bytes.append(single_byte)\n",
    "\n",
    "    single_byte=num+128\n",
    "    out_bytes.append(single_byte)\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    \n",
    "    return out_bytes\n",
    "\n",
    "\n",
    "def vbyte_decode(input_bytes, idx):\n",
    "    \n",
    "    # x stores the decoded number\n",
    "    x = 0\n",
    "    # consumed stores the number of bytes consumed to decode the number\n",
    "    consumed = 0\n",
    "\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    \"\"\"\n",
    "    For each byte in bytes list, decode to decimal and update x. The byte larger than 128 represents the last byte.\n",
    "    \n",
    "    Variable:\n",
    "        s: binary multiplier\n",
    "    \"\"\"\n",
    "    s = 0\n",
    "    input_len = len(input_bytes)\n",
    "    y_int = input_bytes[consumed]\n",
    "    while y_int<128:\n",
    "        consumed+=1\n",
    "        if consumed>idx:\n",
    "            x = x^(y_int<<s)\n",
    "            s+=7\n",
    "            y_int = input_bytes[consumed]\n",
    "        \n",
    "    x=x^((y_int-128)<<s)\n",
    "    consumed+=1\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    \n",
    "    return x, consumed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a sanity check, we ensure that compression and decompression work correctly:\n",
    "for num in range(0, 123456):\n",
    "    vb = vbyte_encode(num)\n",
    "    dec, decoded_bytes = vbyte_decode(vb, 0)\n",
    "    assert(num == dec)\n",
    "    assert(decoded_bytes == len(vb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, you should modify the `InvertedIndex` class to support compression.\n",
    "\n",
    "Your task here is to implement the compression of the `doc_ids` and `doc_term_freqs` lists using the `vbyte_encode` function implemented earlier. Note that the `doc_ids` have to be gap encoded as described in the lecture slides. A helper function `decompress_list` is provided to allow easy decompression of the lists. \n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents = 20000\n",
      "unique terms = 103193\n",
      "longest document = 10576\n",
      "compressed space usage MiB = 7.818\n"
     ]
    }
   ],
   "source": [
    "def decompress_list(input_bytes, gapped_encoded):\n",
    "    res = []\n",
    "    prev = 0\n",
    "    idx = 0\n",
    "    while idx < len(input_bytes):\n",
    "        dec_num, consumed_bytes = vbyte_decode(input_bytes, idx)\n",
    "        idx += consumed_bytes\n",
    "        num = dec_num + prev\n",
    "        res.append(num)\n",
    "        if gapped_encoded:\n",
    "            prev = num\n",
    "    return res\n",
    "\n",
    "class CompressedInvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "        # TODO NOW WE COMPRESS THE LISTS\n",
    "        \n",
    "        ###\n",
    "        # Your answer BEGINS HERE\n",
    "        ###\n",
    "        \"\"\"\n",
    "        Encode the number in self.doc_ids and self.doc_term_freqs\n",
    "        Variable:\n",
    "            term_doc_freq_list: docs frequency binary list for a single term\n",
    "            term_doc_id_list: docs id binary list for a single term\n",
    "        \"\"\"\n",
    "        temp_doc_id = [[] for i in range(len(vocab))]\n",
    "\n",
    "        for i in range(len(self.vocab)):\n",
    "\n",
    "            term_doc_freq_list=[]\n",
    "            for doc_freq in self.doc_term_freqs[i]:\n",
    "                term_doc_freq_list += vbyte_encode(doc_freq)\n",
    "            self.doc_term_freqs[i] = term_doc_freq_list\n",
    "            \n",
    "            term_doc_id_list=[]\n",
    "            prev_doc_id=0\n",
    "            for doc_id in self.doc_ids[i]:\n",
    "                gap_doc = doc_id-prev_doc_id\n",
    "                term_doc_id_list += vbyte_encode(gap_doc)\n",
    "                prev_doc_id = doc_id\n",
    "            self.doc_ids[i] = term_doc_id_list\n",
    "        ###\n",
    "        # Your answer ENDS HERE\n",
    "        ###\n",
    "    \n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        # We decompress\n",
    "        return decompress_list(self.doc_ids[term_id], True)\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        # We decompress\n",
    "        return decompress_list(self.doc_term_freqs[term_id], False)\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # this function assumes the integers are now bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list)\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list)\n",
    "        return space_usage\n",
    "\n",
    "\n",
    "# We output the same statistics as before to ensure we still store the same data but now use much less space\n",
    "compressed_index = CompressedInvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "print(\"documents = {}\".format(compressed_index.num_docs()))\n",
    "print(\"unique terms = {}\".format(compressed_index.num_terms()))\n",
    "print(\"longest document = {}\".format(compressed_index.max_doc_len))\n",
    "print(\"compressed space usage MiB = {:.3f}\".format(compressed_index.space_in_bytes() / (1024.0 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK  1 DOCID     1084 SCORE 1.210 CONTENT Unemployment in South Korea fell to 3.8% of the labor force last year from \n",
      "RANK  2 DOCID      905 SCORE 1.127 CONTENT Seasonally adjusted industrial production in South Korea increased nearly 2\n",
      "RANK  3 DOCID     5612 SCORE 1.056 CONTENT Consumer prices in South Korea rose 2.4% in April from a year-earlier and 0\n",
      "RANK  4 DOCID     9126 SCORE 0.998 CONTENT Foreign investment in South Korea totaled $278 million in 1987's first quar\n",
      "RANK  5 DOCID    17960 SCORE 0.936 CONTENT Henley Group Inc. said its M.W. Kellogg Co. unit received a contract to des\n",
      "RANK  6 DOCID     1760 SCORE 0.926 CONTENT Consumer prices in South Korea rose 1% in February from a year earlier, the\n",
      "RANK  7 DOCID     4132 SCORE 0.926 CONTENT South Korea's trade deficit with Japan grew to a record $629 million in Apr\n",
      "RANK  8 DOCID    17826 SCORE 0.923 CONTENT South Korea revised its 1986 current-account surplus to $5 billion from the\n",
      "RANK  9 DOCID    15803 SCORE 0.911 CONTENT South Korea's 1986 trade surplus with the U.S. was revised upward to $7.41 \n",
      "RANK 10 DOCID    10664 SCORE 0.889 CONTENT South Korea's economy, aided by brisk exports, grew an inflation-adjusted 1\n"
     ]
    }
   ],
   "source": [
    "# Additionally we want to ensure that the index still returns the same results as before\n",
    "query = \"south korea production\"\n",
    "stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "comp_results = query_tfidf(stemmed_query, compressed_index)\n",
    "for rank, res in enumerate(comp_results):\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],raw_docs[res[0]][:75]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
